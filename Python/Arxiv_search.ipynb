{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sentence-similarity to search papers on ArXiV\n",
    "\n",
    "## Are you just starting a project and need to do literature review, but want to do it in a fun way using Large Language Models?\n",
    "\n",
    "One cool application of Large Language Models is to use their [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) to compute text simmilarities. *Embeddings* are machine readable representations of objects that need to be _translated_ for a machine to process them. These representations have numeric values and they attempt to preserve intrinsic properties of the system. \n",
    "\n",
    "![Scheme of a 2-D representation of the words \"statistics\", \"mathematics\", \"Tiger\" and \"Lion\"](https://www.nlplanet.org/course-practical-nlp/_images/word_embeddings.png)\n",
    "| *Fig 1. Scheme of a 2-D representation of the words \"statistics\", \"mathematics\", \"Tiger\" and \"Lion\". Image from:https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/11-text-as-vectors-embeddings* |\n",
    "\n",
    "In the case of words in text, one of the most important properties that we want to preserve is _semantic relationships_; how words or word types are related to each others. In principle, if we compute the distance between two representations of words we will be able to tell how _similar_ they are(see Fig 1). Of course these _semantic relationships_ are going to be dependent on how the model was trained, and although there is a lot to learn from how to build these embeddings this is not the goal of this post. \n",
    "\n",
    "For this post we only need to know that **we can use the embeddings that LLMs use for other things than making queries (prompting) to a LLM**. \n",
    "\n",
    "In this short demonstration we are going to use language models -that were developed to analyze sentences   ([sentence-transformers](https://www.sbert.net/))- to find articles on Arxiv that might be useful for a particular project, using _keywords_ **and** a description of the project (or abstract) we want to develop.  \n",
    "\n",
    "For practicality and to reduce computational time, we are going to divide the search process into the following steps:\n",
    "\n",
    "* **Step 1:** Filter the articles that cointain specific or general keywords that we want to look for\n",
    "* **Step 2:** Compute similarities between the abstracts from the papers that we collected and the article of reference\n",
    "* **Step 3:** Sort collected articles by similarity values\n",
    "\n",
    "This way we don't need to calculate similarities from articles that don't include our keywords, and we can expect to find useful some of the articles listed at the top.\n",
    "\n",
    "Let's start with importing the libraries we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "#for numerical calculations\n",
    "import numpy as np\n",
    "\n",
    "# for files and directories\n",
    "import os\n",
    "import json\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "#regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to retrieve ArXiV's information we can [download their MetaData Dataset from Kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv). This dataset provides the following information for every article on ArXiV:\n",
    "\n",
    "* id: ArXiv ID (can be used to access the paper)\n",
    "* submitter: Who submitted the paper\n",
    "* authors: Authors of the paper\n",
    "* title: Title of the paper\n",
    "* comments: Additional info, such as number of pages and figures\n",
    "* journal-ref: Information about the journal the paper was published in\n",
    "* doi: [https://www.doi.org](Digital Object Identifier)\n",
    "* abstract: The abstract of the paper\n",
    "* categories: Categories / tags in the ArXiv system\n",
    "* versions: A version history\n",
    "\n",
    "once we have downloaded the dataset, we can either load it or _stream_ it. Since the dataset is aroud ~ 4GB, instead of loading the file we are going to use the _yield_ keyword to iterate over the lines without storing the entire file in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Arxiv metadata dataset ~ 2.3 million of papers\n",
    "\n",
    "# File address\n",
    "#On windows\n",
    "data_file = r\"~\Datasets\\Arxiv_MetaData\\arxiv-metadata-oai-snapshot.json\"\n",
    "#On Linux\n",
    "#data_file = \"~/Data/Arxiv_Metadata/arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "\"\"\" get_metadata(data_file)\n",
    "\n",
    "    Function to get metadata from the json file\n",
    "    Args: data_file: name of the file (with full path if needed)  \n",
    "    Returns: yields the lines from the json file\n",
    "\n",
    "\"\"\"\n",
    "def get_metadata(data_file):\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our function ```get_metadata()``` to print out the data for the first paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0704.0001\n",
      "submitter: Pavel Nadolsky\n",
      "authors: C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\n",
      "title: Calculation of prompt diphoton production cross sections at Tevatron and\n",
      "  LHC energies\n",
      "comments: 37 pages, 15 figures; published version\n",
      "journal-ref: Phys.Rev.D76:013009,2007\n",
      "doi: 10.1103/PhysRevD.76.013009\n",
      "report-no: ANL-HEP-PR-07-12\n",
      "categories: hep-ph\n",
      "license: None\n",
      "abstract:   A fully differential calculation in perturbative quantum chromodynamics is\n",
      "presented for the production of massive photon pairs at hadron colliders. All\n",
      "next-to-leading order perturbative contributions from quark-antiquark,\n",
      "gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\n",
      "all-orders resummation of initial-state gluon radiation valid at\n",
      "next-to-next-to-leading logarithmic accuracy. The region of phase space is\n",
      "specified in which the calculation is most reliable. Good agreement is\n",
      "demonstrated with data from the Fermilab Tevatron, and predictions are made for\n",
      "more detailed tests with CDF and DO data. Predictions are shown for\n",
      "distributions of diphoton pairs produced at the energy of the Large Hadron\n",
      "Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\n",
      "boson are contrasted with those produced from QCD processes at the LHC, showing\n",
      "that enhanced sensitivity to the signal can be obtained with judicious\n",
      "selection of events.\n",
      "\n",
      "versions: [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}]\n",
      "update_date: 2008-11-26\n",
      "authors_parsed: [['Bal√°zs', 'C.', ''], ['Berger', 'E. L.', ''], ['Nadolsky', 'P. M.', ''], ['Yuan', 'C. -P.', '']]\n"
     ]
    }
   ],
   "source": [
    "# get the metadata\n",
    "arxiv_metadata = get_metadata(data_file)\n",
    "\n",
    "# Printing the values of the first paper in the dataset by breaking the loop after the first iteration\n",
    "for paper in arxiv_metadata:\n",
    "    for k, v in json.loads(paper).items():\n",
    "        print(f'{k}: {v}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our code is working so far. Now, lets take as case of study a paper already published in a journal that contains keywords and abstract. \n",
    "\n",
    "Because I find collective behaviour fascinating, and I have always wanted to work on a project that studies collective behaviour in Football (yes, the real one). Let's try to find papers that approach the study of collective behaviour in football in the way researchers study animal systems.\n",
    "\n",
    "We are going to use this really cool paper: [Searching for structure in collective systems](https://link.springer.com/article/10.1007/s12064-020-00311-9) from Twomey et al. where they develop a methodology to quantify coordination and identify the most (and least) coordinated components in multi-individual systems.\n",
    "\n",
    "Now, in order to start our search, lets define some of the variables we are going to use. For this particular case I want to find papers that study Football, so I will make that keyword _a must_ for my search, and I will define some secondary keywords that will help us to find more papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the main keywords\n",
    "keywords_all = ['soccer']\n",
    "\n",
    "#secondary keywords\n",
    "keywords_any = ['collective','behaviour','behavior','human','sports', 'football']\n",
    "\n",
    "#the abstract/description we are going to use as reference\n",
    "abstract_reference = ['From fish schools and bird flocks to biofilms and neural networks, collective systems in nature are made up of many mutually influencing individuals that interact locally to produce large-scale coordinated behavior. Although coordination is central to what it means to behave collectively, measures of large-scale coordination in these systems are ad hoc and system specific. The lack of a common quantitative scale makes broad cross-system comparisons difficult. Here we identify a systemindependent measure of coordination based on an information-theoretic measure of multivariate dependence and show it can be used in practice to give a new view of even classic, well-studied collective systems. Moreover, we use this measure to derive a novel method for finding the most coordinated components within a system and demonstrate how this can be used in practice to reveal intrasystem organizational structure.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for the keywords we can define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" contains_any(keywords, text)\n",
    "\n",
    "    Function that recevies a list of keywords and a text and returns True if any of the keywords is in the text\n",
    "    input: keywords, text: as strings.\n",
    "    returns: True if any of the keywords is in the text, False otherwise.\n",
    "\"\"\"\n",
    "def contains_any(keywords, text):\n",
    "    pattern = r\"\\b(\" + \"|\".join(keywords) + r\")\\b\"\n",
    "    return bool(re.search(pattern, text, flags=re.IGNORECASE))\n",
    "\n",
    "\"\"\" contains_all(keywords, text)\n",
    "    \n",
    "    Function that recevies a list of keywords and a text and returns True if all of the keywords are in the text\n",
    "    input: keywords, text: as strings.\n",
    "    returns: True if all of the keywords are in the text, False otherwise.    \n",
    "    \n",
    "\"\"\"\n",
    "def contains_all(keywords, text):\n",
    "    return all(word in text for word in keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start searching and collecting the MetaData from the papers that include \"soccer\" and/or the secondary keywords in their title or abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_papers = 500 #setting a limit for retrieved papers, just in case we get too many\n",
    "counter_ = 0    #counter for the number of papers retrieved\n",
    "collected_papers = []\n",
    "#getting the metadata\n",
    "arxiv_metadata = get_metadata(data_file)\n",
    "\n",
    "#iterating over the metadata\n",
    "for paper in arxiv_metadata:\n",
    "    #loading the information of the paper\n",
    "    paper_info = json.loads(paper)\n",
    "    #checking if the paper contains the primary keywords\n",
    "    if contains_all(keywords_all, paper_info['abstract'].lower()) or contains_all(keywords_all, paper_info['title'].lower()):\n",
    "        #secondary keywords\n",
    "        if contains_any(keywords_any, paper_info['abstract'].lower()) or contains_any(keywords_any, paper_info['title'].lower()):\n",
    "            #we collect the paper\n",
    "            collected_papers.append(paper_info)\n",
    "            counter_ += 1\n",
    "    #if we have reached the limit we break the loop\n",
    "    if counter_ == num_papers:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers collected: 227\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total papers collected: {len(collected_papers)}\")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we didn't need to break the loop since we only found 227. If we want to retrieve more papers we could modify our keywords (adding more to the secondary keywords array), but we don't need that for this example.\n",
    "\n",
    "Let's print out the first 10 articles that we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movement and Man at the end of the Random Walks\n",
      "\n",
      "New Mechanics of Generic Musculo-Skeletal Injury\n",
      "\n",
      "Using the Sound Card as a Timer\n",
      "\n",
      "Dynamics of tournaments: the soccer case\n",
      "\n",
      "Relative Age Effect in Elite Sports: Methodological Bias or Real\n",
      "  Discrimination?\n",
      "\n",
      "Soccer: is scoring goals a predictable Poissonian process?\n",
      "\n",
      "The Socceral Force\n",
      "\n",
      "Relative locality and the soccer ball problem\n",
      "\n",
      "Archetypal Athletes\n",
      "\n",
      "Learning RoboCup-Keepaway with Kernels\n",
      "\n",
      "Towards Real-Time Summarization of Scheduled Events from Twitter Streams\n",
      "\n",
      "A statistical view on team handball results: home advantage, team\n",
      "  fitness and prediction of match outcomes\n",
      "\n",
      "How does the past of a soccer match influence its future?\n",
      "\n",
      "Quantum Consciousness Soccer Simulator\n",
      "\n",
      "Inferring Team Strengths Using a Discrete Markov Random Field\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(collected_papers[i]['title'] + '\\n') for i in range(15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little inspection we can notice that the articles seem to be related to soccer and even if they might contain some of our keywords, it is not very clear how many (if any) of them are going to be helpful for us. In order to retrieve the _most helpful_ articles for our study, we can now compute how similar their abstracts are with respect to our abstract of reference.\n",
    "\n",
    "First we import the libraries we are going to use for the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries for llms\n",
    "from transformers import *\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding we are going to use is from the model `paraphrase-MiniLM-L6-v2` and details about it can be found [here](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2). This model was trained particularly to derive _semantically meaningful_ sentence embeddings and compare them using [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). If you want to know about how the model was developed you can check the ArXiV paper [here](https://arxiv.org/abs/1908.10084) or check the [documentation](https://www.sbert.net/) for more details about the package `sentence_transformers`. \n",
    "\n",
    "Now let's load and inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\agonzal3\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2\\snapshots\\3bf4ae7445aa77c8daaef06518dd78baffff53c9\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}
   ],
   "source": [
    "# We are going to use one of the models from the sentence-transformers library to get the embeddings of the sentences\n",
    "model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "llm_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first thing we notice is that this model is based on [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) and that the embedding that it uses has 384 dimensions (hidden_size). To corroborate that, we can _encode_ (pass through the model a sentence and get its emmbeding) the abstract we are going to use for reference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Getting the embeddings of the reference abstract\n",
    "abstract_em = llm_model.encode(abstract_reference)\n",
    "# Printing the shape of the embeddings\n",
    "print(np.shape(abstract_em))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this numerical representation of our abstract in the 384-dimensional space of this model, we can quantify how _semantically_ close (or far) are the abstracts for the articles that we retrieved compared with the study we have as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing values\n",
    "sim_values = []\n",
    "collected_titles = []\n",
    "collected_abstracts = []\n",
    "collected_arxiv_ids = []\n",
    "collected_doi = []\n",
    "\n",
    "#iterating over the collected papers\n",
    "for paper in collected_papers:\n",
    "    #storing each of the important values we want to keep\n",
    "    collected_titles.append(paper['title'])\n",
    "    collected_abstracts.append(paper['abstract'])\n",
    "    collected_arxiv_ids.append(paper['id'])\n",
    "    collected_doi.append(paper['doi']) \n",
    "    #finally computing the similarity between abstracts and store it\n",
    "    sim_values.append(float(util.cos_sim(abstract_em, llm_model.encode(paper['abstract']))[0][0].detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do a quick inspection of the first 15 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Stochastic model for football's collective dynamics\",\n",
       "       'Using network science to analyze football passing networks: dynamics,\\n  space, time and the multilayer nature of the game',\n",
       "       'A Continuous-Time Stochastic Process for High-Resolution Network Data in\\n  Sports',\n",
       "       'The Soccer Game, bit by bit: An information-theoretic analysis',\n",
       "       'Modeling ball possession dynamics in the game of football',\n",
       "       'A new method for comparing rankings through complex networks: Model and\\n  analysis of competitiveness of major European soccer leagues',\n",
       "       'Emergent Coordination Through Competition',\n",
       "       'Deep Decision Trees for Discriminative Dictionary Learning with\\n  Adversarial Multi-Agent Trajectories',\n",
       "       'Optimising Long-Term Outcomes using Real-World Fluent Objectives: An\\n  Application to Football',\n",
       "       'Hierarchical and State-based Architectures for Robot Behavior Planning\\n  and Control',\n",
       "       'Primacy & Ranking of UEFA Soccer Teams from Biasing Organizing Rules',\n",
       "       \"Disruptive innovations in RoboCup 2D Soccer Simulation League: from\\n  Cyberoos'98 to Gliders2016\",\n",
       "       'Alterations in Structural Correlation Networks with Prior Concussion in\\n  Collision-Sport Athletes',\n",
       "       'Optimising Game Tactics for Football'], dtype='<U145')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the titles oredered by s with the reference title.\n",
    "np.array(collected_titles)[np.argsort(sim_values)][::-1][0:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at the first five papers, I can confirm that I do recognize some of them because this is a topic I am very interested in. But this search already added a lot of references I wasn't aware of, so I'm excited to read them as well!\n",
    "\n",
    "Finally, we can save the papers we found in a DataFrame and export it as `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# indexing the papers by the similarity value in decreasing order\n",
    "ix_order = np.argsort(sim_values)[::-1]\n",
    "# Creating a pandas DataFrame with the collected data\n",
    "df = pd.DataFrame({'title': np.array(collected_titles)[ix_order], \n",
    "                   'abstract': np.array(collected_abstracts)[ix_order], \n",
    "                   'arxiv_id': np.array(collected_arxiv_ids)[ix_order], \n",
    "                   'doi': np.array(collected_doi)[ix_order], \n",
    "                   'similarity': np.array(sim_values)[ix_order]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe as csv\n",
    "df.to_csv('collective_football_biblio.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
